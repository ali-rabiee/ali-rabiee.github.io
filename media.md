---
title: "Research Media"
layout: single
permalink: /media/
author_profile: true
classes: wide smaller-font
---

## STREAMS: Self-Training Robotic End-to-end Adaptive Multimodal Shared autonomy

STREAMS is a deep reinforcement learning framework that combines environment data and user input to produce smooth, stable end-effector trajectories for assistive robots, achieving a 96% success rate in simulations and 83% in user studies without requiring pre-existing datasets.

<video width="600" height="400" controls>
    <source src="{{ '/assets/videos/STREAMS_demo.mp4' | relative_url }}" type="video/mp4">
    Your browser does not support the video tag. You can <a href="{{ '/assets/videos/STREAMS_demo.mp4' | relative_url }}">download the video</a> instead.
</video>

[GitHub Link](https://github.com/ali-rabiee/STREAMS)


## Adaptive Robotic Control for Users with Severe Impairments using DRL

A system that optimizes mapping from low-DoF inputs to high-dimensional robotic actions, enabling
intuitive control for users with severe impairments. It uses adaptive goal prediction and reinforcement learning to
guide actions in real-time, seamlessly blending user input with autonomous assistance. 

![GIF]({{ '/assets/videos/amplification_demo.gif' | relative_url }})

[GitHub Link](https://github.com/ali-rabiee/amplification_DRL)


## Dual-Mode Robotic Arm Control with GUI and IMU Integration

This project integrates a graphical user interface (GUI) with an inertial measurement unit (IMU) sensor to provide
dual-mode control of a robotic arm. Users can choose to control the robotic arm by clicking on cursor buttons in the
GUI or by using the IMU sensor to move the cursor for button selection.

<img src="{{ '/assets/videos/ClickMove_demo.gif' | relative_url }}" alt="Project 1 GIF" width="600" height="auto">

[GitHub Link](https://github.com/ali-rabiee/ClickMove-Robotics)


## Real-time Face Orientation Detection

Developed a real-time video processing application that detects human faces and determines the orientation of the
head for robot control applications.

<img src="{{ '/assets/videos/FaceOrientation_demo.gif' | relative_url }}" alt="Project 1 GIF" width="600" height="auto">

[GitHub Link](https://github.com/ali-rabiee/RealTimeFaceOrientation)


## Autonomous vision-based reach-to-grasp DQN agent

An autonomous vision-based reach-to-grasp system using a Deep Q-Network (DQN) agent for robotic arm control. This project utilizes real-time visual input to enable efficient and adaptive grasping in dynamic environments.

<img src="{{ '/assets/videos/auto_grasp.gif' | relative_url }}" alt="Project 1 GIF" width="700" height="auto">

[GitHub Link](https://github.com/ali-rabiee/Auto-Vision-Grasp)

## CartPole solution using DQN
his project implements a Deep Q-Network (DQN) to solve the CartPole-v1 environment from OpenAI Gym. The CartPole problem involves balancing a pole on a moving cart. This implementation uses a DQN to learn a policy that keeps the pole balanced for as long as possible.

<img src="{{ '/assets/videos/cartpole.gif' | relative_url }}" alt="Project 1 GIF" width="700" height="auto">

[GitHub Link](https://github.com/ali-rabiee/CartPole-DQN)